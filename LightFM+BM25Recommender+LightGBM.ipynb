{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from lightfm import LightFM\n",
    "\n",
    "from implicit.nearest_neighbours import BM25Recommender\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "def average_precision(\n",
    "        dict data_true,\n",
    "        dict data_predicted,\n",
    "        const unsigned long int k\n",
    ") -> float:\n",
    "    cdef:\n",
    "        unsigned long int n_items_predicted\n",
    "        unsigned long int n_items_true\n",
    "        unsigned long int n_correct_items\n",
    "        unsigned long int item_idx\n",
    "\n",
    "        double average_precision_sum\n",
    "        double precision\n",
    "\n",
    "        set items_true\n",
    "        list items_predicted\n",
    "\n",
    "    if not data_true:\n",
    "        raise ValueError('data_true is empty')\n",
    "\n",
    "    average_precision_sum = 0.0\n",
    "\n",
    "    for key, items_true in data_true.items():\n",
    "        items_predicted = data_predicted.get(key, [])\n",
    "\n",
    "        n_items_true = len(items_true)\n",
    "        n_items_predicted = min(len(items_predicted), k)\n",
    "\n",
    "        if n_items_true == 0 or n_items_predicted == 0:\n",
    "            continue\n",
    "\n",
    "        n_correct_items = 0\n",
    "        precision = 0.0\n",
    "\n",
    "        for item_idx in range(n_items_predicted):\n",
    "            if items_predicted[item_idx] in items_true:\n",
    "                n_correct_items += 1\n",
    "                precision += <double>n_correct_items / <double>(item_idx + 1)\n",
    "\n",
    "        average_precision_sum += <double>precision / <double>min(n_items_true, k)\n",
    "\n",
    "    return average_precision_sum / <double>len(data_true)\n",
    "\n",
    "def metric(true_data, predicted_data, k=20):\n",
    "    true_data_set = {k: set(v) for k, v in true_data.items()}\n",
    "\n",
    "    return average_precision(true_data_set, predicted_data, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pickle.load(open(os.path.join(DATA_PATH, \"transactions.pkl\"), \"rb\"))\n",
    "transactions = transactions[[\"ts\", \"rating\", \"label\"]]\n",
    "transactions.reset_index([0, 1], inplace=True)\n",
    "transactions.element_uid = transactions.element_uid.astype(np.int16)\n",
    "transactions.user_uid = transactions.user_uid.astype(np.int32)\n",
    "transactions.rating = transactions.rating.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks = pd.read_pickle(os.path.join(DATA_PATH, \"bookmarks.pkl\"))\n",
    "bookmarks.element_uid = bookmarks.element_uid.astype(np.int16)\n",
    "bookmarks.user_uid = bookmarks.user_uid.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "user_encoder.fit(transactions.user_uid)\n",
    "element_encoder = LabelEncoder()\n",
    "element_encoder.fit(transactions.element_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8 = transactions.ts.quantile(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPart:\n",
    "\n",
    "    def __init__(self, transactions, bookmarks, qs, qe=1):\n",
    "        q1 = transactions.ts.quantile(qs)\n",
    "        q2 = transactions.ts.quantile(qe)\n",
    "        #разобьем интеракции по времени на train и test\n",
    "        transactions_train = transactions[transactions.ts <= q1]\n",
    "        transactions_test = transactions[(transactions.ts > q1) & (transactions.ts <= q2)]\n",
    "        \n",
    "        bookmarks = bookmarks[bookmarks.ts <= q1][[\"user_uid\", \"element_uid\"]]\n",
    "\n",
    "        train_users = transactions_train.user_uid.unique()\n",
    "        transactions_test = transactions_test[transactions_test.user_uid.isin(train_users)]\n",
    "\n",
    "\n",
    "        transactions_train.user_uid = user_encoder.transform(transactions_train.user_uid).astype(np.int32)\n",
    "        transactions_test.user_uid = user_encoder.transform(transactions_test.user_uid).astype(np.int32)\n",
    "        bookmarks.user_uid = user_encoder.transform(bookmarks.user_uid).astype(np.int32)\n",
    "\n",
    "        transactions_train.element_uid = element_encoder.transform(transactions_train.element_uid).astype(np.int16)\n",
    "        transactions_test.element_uid = element_encoder.transform(transactions_test.element_uid).astype(np.int16)\n",
    "        bookmarks.element_uid = element_encoder.transform(bookmarks.element_uid).astype(np.int16)\n",
    "        #delete_interactions_df - для фильтрации интеракций при валидации моделей первого уровня\n",
    "        delete_interactions_df = transactions_train[transactions_train.label != -1].drop_duplicates()\n",
    "        \n",
    "        # соберем признаки - дата первой интеракци, дата последней интеракции\n",
    "        min_time = transactions_train.ts.min()\n",
    "        transactions_train.ts = transactions_train.ts.apply(lambda x: round((x - min_time) / (3600 * 12)))\n",
    "\n",
    "        user_ts = transactions_train.groupby(\"user_uid\")[\"ts\"].agg({\"min_ts\":min, \"max_ts\":max})\n",
    "        max_abs = user_ts.max_ts.max()\n",
    "        user_ts[\"diff_ts\"] = user_ts.max_ts - user_ts.min_ts\n",
    "        user_ts = user_ts / max_abs   \n",
    "        self.user_ts = user_ts.copy()\n",
    "        \n",
    "        transactions_train.drop(columns=[\"ts\"], inplace=True)\n",
    "        transactions_test.drop(columns=[\"ts\"], inplace=True)\n",
    "        #просуммируем рейтинг по разным интеракиям.\n",
    "        #Например, фильм могли сначала добавить в закладки, а затем посмотреть, и поставить оценку\n",
    "        transactions_train.rating = transactions_train.groupby([\"user_uid\", \"element_uid\"],\n",
    "                                                               as_index=True)[\"rating\"].transform(\"sum\")\n",
    "        transactions_train.label = transactions_train.groupby([\"element_uid\", \"user_uid\"],\n",
    "                                                               as_index=True)[\"label\"].transform(\"max\")\n",
    "        transactions_train = transactions_train.drop_duplicates()\n",
    "        \n",
    "\n",
    "        transactions_test.rating = transactions_test.groupby([\"user_uid\", \"element_uid\"],\n",
    "                                                               as_index=True)[\"rating\"].transform(\"sum\")\n",
    "        transactions_test.label = transactions_test.groupby([\"element_uid\", \"user_uid\"],\n",
    "                                                               as_index=True)[\"label\"].transform(\"max\")\n",
    "        transactions_test = transactions_test.drop_duplicates()\n",
    "        #выбрем из закладок те, которые пользователь не успел просмотреть за период в train.\n",
    "        transactions_made = transactions_train[transactions_train.label != -1].set_index([\"user_uid\", \"element_uid\"])\n",
    "        bookmarks.set_index([\"user_uid\", \"element_uid\"], inplace=True)\n",
    "        bookmarks = bookmarks.loc[bookmarks.index.difference(transactions_made.index)]\n",
    "        bookmarks_set = bookmarks.reset_index([0, 1]).groupby(\"user_uid\")[\"element_uid\"].apply(set).to_dict()\n",
    "        # исключим из выборки пользователей, которые не потребили ни одной единицы контента\n",
    "        user_cnt = transactions_test[transactions_test.label == 1].user_uid.value_counts()\n",
    "        user_rear = set(user_cnt[user_cnt == 1].index)\n",
    "        \n",
    "        self.user_cnt = user_cnt.to_dict().copy()\n",
    "\n",
    "        users_test = set(transactions_test[transactions_test.label == 1].user_uid.unique().astype(np.int32))\n",
    "        users_test = sorted(list(users_test.difference(user_rear)))\n",
    "        elements_test = list(range(len(element_encoder.classes_)))\n",
    "        \n",
    "        #Удалим контент, кооторый посмотрели менее 100 раз.\n",
    "        element_cnt = transactions_train.element_uid.value_counts()\n",
    "        rear_train = set(list(element_cnt[element_cnt < 100].index))\n",
    "        # user_test_filter - для фильтрации топа, после моделей первого уровня\n",
    "        user_test_filter = delete_interactions_df[delete_interactions_df.user_uid.isin(users_test)].\\\n",
    "        groupby(\"user_uid\")[\"element_uid\"].apply(set).to_dict()\n",
    "        \n",
    "        user_test_filter = {user: user_test_filter.get(user, set()).union(rear_train).difference(bookmarks_set.get(user, set())) \\\n",
    "                                 for user in users_test}\n",
    "        \n",
    "            \n",
    "        self.user_test_filter = user_test_filter.copy()\n",
    "        #user_test_true - для валидации модели второго уровня\n",
    "        user_test_true = transactions_test[transactions_test.label == 1].groupby(\"user_uid\")[\"element_uid\"].apply(set)\n",
    "        user_test_true_df = transactions_test[transactions_test.label==1][[\"user_uid\", \"element_uid\", \"label\"]].drop_duplicates()\n",
    "        user_test_true_df.set_index([\"user_uid\", \"element_uid\"], inplace=True)\n",
    "        self.user_test_true_df = user_test_true_df.copy()\n",
    "\n",
    "        self.user_test_true = user_test_true.to_dict()\n",
    "        #sparse матрицы для обучения/валидации моделей первого уровня\n",
    "        self.X_train = sp.coo_matrix((list(transactions_train.rating + 0.01),\n",
    "                                (list(transactions_train.user_uid), list(transactions_train.element_uid))))\n",
    "\n",
    "        self.X_del = sp.coo_matrix((list(delete_interactions_df.rating),\n",
    "                                (list(delete_interactions_df.user_uid), list(delete_interactions_df.element_uid))))\n",
    "\n",
    "        which_test = transactions_test.label == 1\n",
    "        self.X_test = sp.coo_matrix((list(transactions_test[which_test].rating),\n",
    "                                (list(transactions_test[which_test].user_uid),\n",
    "                                 list(transactions_test[which_test].element_uid))))\n",
    "        \n",
    "        self.transactions_test = transactions_test.copy()\n",
    "        self.transactions_train = transactions_train.copy()\n",
    "        self.delete_interactions_df = delete_interactions_df.copy()\n",
    "        self.users_test = users_test.copy()\n",
    "        self.elements_test = elements_test.copy()\n",
    "        self.bookmarks = bookmarks.copy()\n",
    "        self.rear_train = rear_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_q8 = DataPart(transactions, bookmarks,  0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычеслиения map для LightFM\n",
    "def compute_map(data):\n",
    "    ranks = data.model_mf.predict_rank(data.X_test, data.X_del, num_threads=80, check_intersections=False)\n",
    "    mask = ranks.copy()\n",
    "    mask.data = np.less(mask.data, 20, mask.data)\n",
    "    ranks.data += 1\n",
    "    ranks.data = ranks.data * mask.data\n",
    "    ranks.eliminate_zeros()\n",
    "    ranks = ranks.tolil().data\n",
    "    average_precision_sum = 0.0\n",
    "    for x in data.indices:\n",
    "        n_correct_items = 0\n",
    "        precision = 0\n",
    "        for y in sorted(ranks[x]):\n",
    "            n_correct_items += 1\n",
    "            precision += n_correct_items / y\n",
    "        average_precision_sum += precision / min(data.total[x], 20)\n",
    "    average_precision_sum /= len(data.indices)\n",
    "    return average_precision_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_lfm(data):\n",
    "    maps = []\n",
    "    data.model_mf = LightFM(no_components=200, loss='warp', learning_schedule='adagrad', user_alpha=6e-5,\n",
    "                       item_alpha=2e-5, learning_rate=0.01, max_sampled=150)\n",
    "    data.total = data.X_test.getnnz(axis=1)\n",
    "    data.indices = np.nonzero(data.total)[0]\n",
    "    for i in tqdm(range(10)):\n",
    "        data.model_mf.fit_partial(data.X_train, sample_weight=data.X_train, epochs=10, num_threads=80)\n",
    "        maps.append(compute_map(data))\n",
    "        print(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_lfm(data_q8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#из модели можно взять вектора пользователей для предсказания на LB\n",
    "pickle.dump(data_q8.model_mf, open(\"model_mf_train_q8.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bm25(data):\n",
    "    data.model_bm25 = BM25Recommender(K=150, B=0.8)\n",
    "    data.model_bm25.fit(data.X_train.T.tocsr().astype(np.float32))\n",
    "    \n",
    "make_bm25(data_q8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберем топ рекомендаций от моделей первого уровня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_df(data, topk=200):\n",
    "    data.X_train = data.X_train.tocsr()\n",
    "    data.bm25_dict = dict()\n",
    "    data.bm25_pairs = []\n",
    "    data.bm25_user_sets = dict()\n",
    "    for user in tqdm(data.users_test):\n",
    "#       \n",
    "        rec_current = data.model_bm25.recommend(user, data.X_train, N=10000,\n",
    "                                                filter_already_liked_items=False)\n",
    "        \n",
    "        \n",
    "        current_extend = []\n",
    "        current_set = set()\n",
    "        filter_current = set(data.user_test_filter.get(user, []))\n",
    "        \n",
    "        for rank, rec in enumerate(rec_current):\n",
    "            # дополнительно отфильтруем рекомендации с неположительным скором\n",
    "            if not rec[0] in filter_current and rec[1] > 0:\n",
    "                data.bm25_dict[(user, rec[0])] = (rec[1], rank + 1)\n",
    "                if len(current_extend) < topk:\n",
    "                    current_extend.append((user, rec[0]))\n",
    "                    current_set.add(rec[0])\n",
    "                if len(current_extend) >= topk:\n",
    "                    break\n",
    "        data.bm25_pairs.extend(current_extend)\n",
    "        data.bm25_user_sets[user] = current_set\n",
    "\n",
    "get_bm25_df(data_q8, topk=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfm_df(data, topk=200, test_mode=False):\n",
    "    user_biases = data.model_mf.user_biases[data.users_test]\n",
    "    item_biases = data.model_mf.item_biases[data.elements_test]\n",
    "\n",
    "    user_embeddings = data.model_mf.user_embeddings[data.users_test]\n",
    "    item_embeddings = data.model_mf.item_embeddings[data.elements_test]\n",
    "\n",
    "    lightfm_dot_product = user_embeddings.dot(item_embeddings.T)\n",
    "    lightfm_prediction = lightfm_dot_product + user_biases.reshape(-1, 1) + item_biases.reshape(1, -1)\n",
    "    lightfm_prediction_elements = (-lightfm_prediction).argsort(axis=1)\n",
    "    lightfm_prediction_values = -np.sort(-lightfm_prediction, axis=1)\n",
    "    \n",
    "\n",
    "    elements_lightfm = dict(list(zip(data.users_test, lightfm_prediction_elements)))\n",
    "    values_lightfm = dict(list(zip(data.users_test, lightfm_prediction_values)))\n",
    "    \n",
    "    data.user_biases_series = pd.Series(user_biases, index=data.users_test, name=\"user_bias\")\n",
    "    data.item_biases_series = pd.Series(item_biases, index=data.elements_test, name=\"item_bias\")\n",
    "    \n",
    "    data.lighfm_dict = dict()\n",
    "    data.lightfm_pairs = []\n",
    "    for user_id, user in tqdm(enumerate(data.users_test)):\n",
    "        current_extend = []\n",
    "        current_values = values_lightfm[user]\n",
    "\n",
    "        filter_current = data.user_test_filter.get(user, set())\n",
    "\n",
    "        for rank, (element, value) in enumerate(zip(elements_lightfm[user], current_values)):\n",
    "            if not element in filter_current:\n",
    "                if len(current_extend) < topk:\n",
    "                    data.lighfm_dict[(user, element)] =  (value, rank + 1)\n",
    "                    current_extend.append((user, element))\n",
    "                if len(current_extend) >= topk:\n",
    "                    break\n",
    "        \n",
    "        data.lightfm_pairs.extend(current_extend)\n",
    "\n",
    "    data.user_embeddings = pd.DataFrame(data.model_mf.user_embeddings[data.users_test], index=data.users_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_lfm_df(data_q8, topk=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data\", 'catalogue.json'), 'r') as f:\n",
    "    catalogue = json.load(f)\n",
    "    \n",
    "catalog = pd.DataFrame({int(k): v for k, v in catalogue.items()}).transpose()\n",
    "\n",
    "for feature in [\"purchase\", \"rent\", \"subscription\"]:\n",
    "    catalog[feature] = catalog.availability.apply(lambda x: feature in x).astype(int)\n",
    "catalog.drop(columns=[\"availability\", \"attributes\"], inplace=True)\n",
    "\n",
    "catalog.duration += 5\n",
    "\n",
    "\n",
    "type_encoder = LabelEncoder()\n",
    "catalog[\"type\"] = type_encoder.fit_transform(catalog[\"type\"])\n",
    "\n",
    "for column in [\"duration\", \"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"feature_5\"]:\n",
    "    catalog[column] = catalog[column].astype(float)\n",
    "catalog.drop(columns=[\"feature_1\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_join(data):\n",
    "    all_pairs = set(data.bm25_pairs).union(set(data.lightfm_pairs))\n",
    "    data.user_test_true_df = data.user_test_true_df.label.to_dict()\n",
    "    data.bookmarks[\"bookmark\"] = 1\n",
    "    data.bookmarks = data.bookmarks.bookmark.to_dict()\n",
    "    \n",
    "    X = []\n",
    "    for pair in tqdm(all_pairs):\n",
    "        current = pair + data.bm25_dict.get(pair, (np.nan, np.nan)) + data.lighfm_dict.get(pair, (np.nan, np.nan))\n",
    "        current = current + (data.user_test_true_df.get(pair, 0), ) + (data.bookmarks.get(pair, 0), )\n",
    "        X.append(current)\n",
    "    \n",
    "    X = pd.DataFrame(X, columns = [\"user_uid\", \"element_uid\", \"bm25_v\", \"bm25_r\",\n",
    "                                   \"lfm_v\", \"lfm_r\", \"label\", \"bookmark\"])\n",
    "    X.bm25_r.fillna(2000, inplace=True)\n",
    "    X.lfm_r.fillna(2000, inplace=True)\n",
    "    \n",
    "    catalog_copy = catalog.copy()\n",
    "    catalog_copy = catalog_copy.loc[catalog_copy.index.intersection(pd.Index(element_encoder.classes_))]\n",
    "    catalog_copy.index.names = [\"element_uid\"]\n",
    "    catalog_copy.reset_index(0, inplace=True)\n",
    "    catalog_copy.element_uid = element_encoder.transform(catalog_copy.element_uid).astype(np.int16)\n",
    "    \n",
    "    X = X.merge(catalog_copy, on=\"element_uid\", how=\"left\")\n",
    "    X = X.merge(data.user_biases_series, left_on=\"user_uid\", right_index=True)\n",
    "    X = X.merge(data.item_biases_series, left_on=\"element_uid\", right_index=True)\n",
    "    X = X.merge(data.user_embeddings, left_on=\"user_uid\", right_index=True, how=\"left\")\n",
    "    X = X.merge(data.user_ts, left_on=\"user_uid\", right_index=True, how=\"left\")\n",
    "\n",
    "    user_with_pos = X[X.label == 1].user_uid.unique()\n",
    "    element_with_pos = X[X.label == 1].element_uid.unique()\n",
    "    X = X[X.user_uid.isin(user_with_pos)]\n",
    "    X = X[X.element_uid.isin(element_with_pos)]\n",
    "    X.set_index(\"element_uid\", inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_join(data_q8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_train, users_test = train_test_split(X.user_uid.unique(), random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[X.user_uid.isin(users_train)]\n",
    "X_test = X[X.user_uid.isin(users_test)]\n",
    "y_train = X_train.pop(\"label\")\n",
    "y_test = X_test.pop(\"label\")\n",
    "\n",
    "X_train = X_train.reset_index(0).set_index([\"user_uid\", \"element_uid\"])\n",
    "X_test = X_test.reset_index(0).set_index([\"user_uid\", \"element_uid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "train_data = lightgbm.Dataset(lgb_train, y_train)\n",
    "test_data = lightgbm.Dataset(lgb_test, y_test)\n",
    "\n",
    "parameters = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"min_data_in_leaf\": 80,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    'verbose': 1,\n",
    "    \"num_threads\": 20,\n",
    "    \"lambda_l1\": 0.5,\n",
    "    \"lambda_l2\": 0.1,\n",
    "}\n",
    "\n",
    "model = lightgbm.train(parameters,\n",
    "                       train_data,\n",
    "                       categorical_feature=categorical_feature,\n",
    "                       valid_sets=test_data,\n",
    "                       num_boost_round=3000,\n",
    "                       early_stopping_rounds=100,\n",
    "                      verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_test[\"lgb_score\"] = model.predict(lgb_test, num_iteration=model.best_iteration)\n",
    "lgb_res = lgb_test.reset_index([0, 1])[[\"user_uid\",\n",
    "                                        \"element_uid\",\n",
    "                                        \"lgb_score\"]].sort_values(\"lgb_score\",\n",
    "                                                                  ascending=False)\n",
    "user_elements = dict()\n",
    "for user, group in tqdm(lgb_res.groupby(\"user_uid\")):\n",
    "    user_elements[user] = list(group.element_uid)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = {user:data_q8.user_test_true[user] for user in users_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric(test_true, user_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
